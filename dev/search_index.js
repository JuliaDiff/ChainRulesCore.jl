var documenterSearchIndex = {"docs":
[{"location":"complex.html#How-do-chain-rules-work-for-complex-functions?","page":"Complex Numbers","title":"How do chain rules work for complex functions?","text":"","category":"section"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"ChainRules follows the convention that frule applied to a function f(x + i y) = u(xy) + i v(xy) with perturbation Delta x + i Delta y returns the value and","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"tfracpartial upartial x  Delta x + tfracpartial upartial y  Delta y + i  Bigl( tfracpartial vpartial x  Delta x + tfracpartial vpartial y  Delta y Bigr)\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"Similarly, rrule applied to the same function returns the value and a pullback function which, when applied to the adjoint Delta u + i Delta v, returns","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"Delta u  tfracpartial upartial x + Delta v  tfracpartial vpartial x + i  Bigl(Delta u  tfracpartial u partial y + Delta v  tfracpartial vpartial y Bigr)\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"If we interpret complex numbers as vectors in mathbbR^2, then frule (rrule) corresponds to multiplication with the (transposed) Jacobian of f(z), i.e. frule corresponds to","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"beginpmatrix\ntfracpartial upartial x  Delta x + tfracpartial upartial y  Delta y\n\ntfracpartial vpartial x  Delta x + tfracpartial vpartial y  Delta y\nendpmatrix\n=\nbeginpmatrix\ntfracpartial upartial x  tfracpartial upartial y \ntfracpartial vpartial x  tfracpartial vpartial y \nendpmatrix\nbeginpmatrix\nDelta x  Delta y\nendpmatrix\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"and rrule corresponds to","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"beginpmatrix\ntfracpartial upartial x  Delta u + tfracpartial vpartial x  Delta v\n\ntfracpartial upartial y  Delta u + tfracpartial vpartial y  Delta v\nendpmatrix\n=\nbeginpmatrix\ntfracpartial upartial x  tfracpartial upartial y \ntfracpartial vpartial x  tfracpartial vpartial y \nendpmatrix^T\nbeginpmatrix\nDelta u  Delta v\nendpmatrix\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"The Jacobian of fmathbbC to mathbbC interpreted as a function mathbbR^2 to mathbbR^2 can hence be evaluated using either of the following functions.","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function jacobian_via_frule(f,z)\n    du_dx, dv_dx = reim(frule((Zero(), 1),f,z)[2])\n    du_dy, dv_dy = reim(frule((Zero(),im),f,z)[2])\n    return [\n        du_dx  du_dy\n        dv_dx  dv_dy\n    ]\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function jacobian_via_rrule(f,z)\n    _, pullback = rrule(f,z)\n    du_dx, du_dy = reim(pullback( 1)[2])\n    dv_dx, dv_dy = reim(pullback(im)[2])\n    return [\n        du_dx  du_dy\n        dv_dx  dv_dy\n    ]\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"If f(z) is holomorphic, then the derivative part of frule can be implemented as f(z)  Delta z and the derivative part of rrule can be implemented as operatornameconjbigl(f(z)bigr)  Delta f. Consequently, holomorphic derivatives can be evaluated using either of the following functions.","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function holomorphic_derivative_via_frule(f,z)\n    fz,df_dz = frule((Zero(),1),f,z)\n    return df_dz\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function holomorphic_derivative_via_rrule(f,z)\n    fz, pullback = rrule(f,z)\n    dself, conj_df_dz = pullback(1)\n    return conj(conj_df_dz)\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"note: Note\nThere are various notions of complex derivatives (holomorphic and Wirtinger derivatives, Jacobians, gradients, etc.) which differ in subtle but important ways. The goal of ChainRules is to provide the basic differentiation rules upon which these derivatives can be implemented, but it does not implement these derivatives itself. It is recommended that you carefully check how the above definitions of frule and rrule translate into your specific notion of complex derivative, since getting this wrong will quietly give you wrong results.","category":"page"},{"location":"api.html#API-Documentation","page":"API","title":"API Documentation","text":"","category":"section"},{"location":"api.html#Rules","page":"API","title":"Rules","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\"rules.jl\"]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.frule-Tuple{Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.frule","text":"frule((Δf, Δx...), f, x...)\n\nExpressing the output of f(x...) as Ω, return the tuple:\n\n(Ω, ΔΩ)\n\nThe second return value is the differential w.r.t. the output.\n\nIf no method matching frule((Δf, Δx...), f, x...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> dself = NO_FIELDS;\n\njulia> x = rand()\n0.8236475079774124\n\njulia> sinx, Δsinx = frule((dself, 1), sin, x)\n(0.7336293678134624, 0.6795498147167869)\n\njulia> sinx == sin(x)\ntrue\n\njulia> Δsinx == cos(x)\ntrue\n\nUnary input, binary output scalar function:\n\njulia> sincosx, Δsincosx = frule((dself, 1), sincos, x);\n\njulia> sincosx == sincos(x)\ntrue\n\njulia> Δsincosx[1] == cos(x)\ntrue\n\njulia> Δsincosx[2] == -sin(x)\ntrue\n\nNote that techically speaking julia does not have multiple output functions, just functions that return a single output that is iterable, like a Tuple. So this is actually a Composite:\n\njulia> Δsincosx\nComposite{Tuple{Float64,Float64}}(0.6795498147167869, -0.7336293678134624)\n\n.\n\nSee also: rrule, @scalar_rule\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.rrule-Tuple{Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.rrule","text":"rrule(f, x...)\n\nExpressing x as the tuple (x₁, x₂, ...) and the output tuple of f(x...) as Ω, return the tuple:\n\n(Ω, (Ω̄₁, Ω̄₂, ...) -> (s̄elf, x̄₁, x̄₂, ...))\n\nWhere the second return value is the the propagation rule or pullback. It takes in differentials corresponding to the outputs (x̄₁, x̄₂, ...), and s̄elf, the internal values of the function itself (for closures)\n\nIf no method matching rrule(f, xs...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> x = rand();\n\njulia> sinx, sin_pullback = rrule(sin, x);\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pullback(1) == (NO_FIELDS, cos(x))\ntrue\n\nbinary input, unary output scalar function:\n\njulia> x, y = rand(2);\n\njulia> hypotxy, hypot_pullback = rrule(hypot, x, y);\n\njulia> hypotxy == hypot(x, y)\ntrue\n\njulia> hypot_pullback(1) == (NO_FIELDS, (x / hypot(x, y)), (y / hypot(x, y)))\ntrue\n\nSee also: frule, @scalar_rule\n\n\n\n\n\n","category":"method"},{"location":"api.html#Rule-Definition-Tools","page":"API","title":"Rule Definition Tools","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\"rule_definition_tools.jl\"]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.@non_differentiable-Tuple{Any}","page":"API","title":"ChainRulesCore.@non_differentiable","text":"@non_differentiable(signature_expression)\n\nA helper to make it easier to declare that a method is not not differentiable. This is a short-hand for defining an frule and rrule that return DoesNotExist() for all partials (except for the function s̄elf-partial itself which is NO_FIELDS)\n\nKeyword arguments should not be included.\n\njulia> @non_differentiable Base.:(==)(a, b)\n\njulia> _, pullback = rrule(==, 2.0, 3.0);\n\njulia> pullback(1.0)\n(Zero(), DoesNotExist(), DoesNotExist())\n\nYou can place type-constraints in the signature:\n\njulia> @non_differentiable Base.length(xs::Union{Number, Array})\n\njulia> frule((Zero(), 1), length, [2.0, 3.0])\n(2, DoesNotExist())\n\nwarning: Warning\nThis helper macro covers only the simple common cases. It does not support Varargs, or where-clauses. For these you can declare the rrule and frule directly\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.@scalar_rule-Tuple{Any,Any,Vararg{Any,N} where N}","page":"API","title":"ChainRulesCore.@scalar_rule","text":"@scalar_rule(f(x₁, x₂, ...),\n             @setup(statement₁, statement₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nA convenience macro that generates simple scalar forward or reverse rules using the provided partial derivatives. Specifically, generates the corresponding methods for frule and rrule:\n\nfunction ChainRulesCore.frule((NO_FIELDS, Δx₁, Δx₂, ...), ::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, (\n            (∂f₁_∂x₁ * Δx₁ + ∂f₁_∂x₂ * Δx₂ + ...),\n            (∂f₂_∂x₁ * Δx₁ + ∂f₂_∂x₂ * Δx₂ + ...),\n            ...\n        )\nend\n\nfunction ChainRulesCore.rrule(::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, ((ΔΩ₁, ΔΩ₂, ...)) -> (\n            NO_FIELDS,\n            ∂f₁_∂x₁ * ΔΩ₁ + ∂f₂_∂x₁ * ΔΩ₂ + ...),\n            ∂f₁_∂x₂ * ΔΩ₁ + ∂f₂_∂x₂ * ΔΩ₂ + ...),\n            ...\n        )\nend\n\nIf no type constraints in f(x₁, x₂, ...) within the call to @scalar_rule are provided, each parameter in the resulting frule/rrule definition is given a type constraint of Number. Constraints may also be explicitly be provided to override the Number constraint, e.g. f(x₁::Complex, x₂), which will constrain x₁ to Complex and x₂ to Number.\n\nAt present this does not support defining for closures/functors. Thus in reverse-mode, the first returned partial, representing the derivative with respect to the function itself, is always NO_FIELDS. And in forward-mode, the first input to the returned propagator is always ignored.\n\nThe result of f(x₁, x₂, ...) is automatically bound to Ω. This allows the primal result to be conveniently referenced (as Ω) within the derivative/setup expressions.\n\nThis macro assumes complex functions are holomorphic. In general, for non-holomorphic functions, the frule and rrule must be defined manually.\n\nThe @setup argument can be elided if no setup code is need. In other words:\n\n@scalar_rule(f(x₁, x₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nis equivalent to:\n\n@scalar_rule(f(x₁, x₂, ...),\n             @setup(nothing),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nFor examples, see ChainRules' rulesets directory.\n\nSee also: frule, rrule.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#Differentials","page":"API","title":"Differentials","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\n    \"differentials/abstract_zero.jl\",\n    \"differentials/one.jl\",\n    \"differentials/composite.jl\",\n    \"differentials/thunks.jl\",\n    \"differentials/abstract_differential.jl\",\n]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.AbstractZero","page":"API","title":"ChainRulesCore.AbstractZero","text":"AbstractZero <: AbstractDifferential\n\nSupertype for zero-like differentials—i.e., differentials that act like zero when added or multiplied to other values. If an AD system encounters a propagator that takes as input only subtypes of AbstractZero, then it can stop performing AD operations. All propagators are linear functions, and thus the final result will be zero.\n\nAll AbstractZero subtypes are singleton types. There are two of them: Zero() and DoesNotExist().\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.DoesNotExist","page":"API","title":"ChainRulesCore.DoesNotExist","text":"DoesNotExist() <: AbstractZero\n\nThis differential indicates that the derivative does not exist. It is the differential for primal types that are not differentiable, such as integers or booleans (when they are not being used to represent floating-point values). The only valid way to perturb such values is to not change them at all. As a consequence, DoesNotExist is functionally identical to Zero(), but it provides additional semantic information.\n\nAdding this differential to a primal is generally wrong: gradient-based methods cannot be used to optimize over discrete variables. An optimization package making use of this might want to check for such a case.\n\n!!! note:     This does not indicate that the derivative is not implemented,     but rather that mathematically it is not defined.\n\nThis mostly shows up as the derivative with respect to dimension, index, or size arguments.\n\n    function rrule(fill, x, len::Int)\n        y = fill(x, len)\n        fill_pullback(ȳ) = (NO_FIELDS, @thunk(sum(Ȳ)), DoesNotExist())\n        return y, fill_pullback\n    end\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Zero","page":"API","title":"ChainRulesCore.Zero","text":"Zero() <: AbstractZero\n\nThe additive identity for differentials. This is basically the same as 0. A derivative of Zero() does not propagate through the primal function.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.One","page":"API","title":"ChainRulesCore.One","text":" One()\n\nThe Differential which is the multiplicative identity. Basically, this represents 1.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.NO_FIELDS","page":"API","title":"ChainRulesCore.NO_FIELDS","text":"NO_FIELDS\n\nConstant for the reverse-mode derivative with respect to a structure that has no fields. The most notable use for this is for the reverse-mode derivative with respect to the function itself, when that function is not a closure.\n\n\n\n\n\n","category":"constant"},{"location":"api.html#ChainRulesCore.Composite","page":"API","title":"ChainRulesCore.Composite","text":"Composite{P, T} <: AbstractDifferential\n\nThis type represents the differential for a struct/NamedTuple, or Tuple. P is the the corresponding primal type that this is a differential for.\n\nComposite{P} should have fields (technically properties), that match to a subset of the fields of the primal type; and each should be a differential type matching to the primal type of that field. Fields of the P that are not present in the Composite are treated as Zero.\n\nT is an implementation detail representing the backing data structure. For Tuple it will be a Tuple, and for everything else it will be a NamedTuple. It should not be passed in by user.\n\nFor Composites of Tuples, iterate and getindex are overloaded to behave similarly to for a tuple. For Composites of structs, getproperty is overloaded to allow for accessing values via comp.fieldname. Any fields not explictly present in the Composite are treated as being set to Zero(). To make a Composite have all the fields of the primal the canonicalize function is provided.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.canonicalize-Union{Tuple{Composite{P,var\"#s14\"} where var\"#s14\"<:(NamedTuple{L,T} where T<:Tuple)}, Tuple{L}, Tuple{P}} where L where P","page":"API","title":"ChainRulesCore.canonicalize","text":"canonicalize(comp::Composite{P}) -> Composite{P}\n\nReturn the canonical Composite for the primal type P. The property names of the returned Composite match the field names of the primal, and all fields of P not present in the input comp are explictly set to Zero().\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.InplaceableThunk","page":"API","title":"ChainRulesCore.InplaceableThunk","text":"InplaceableThunk(val::Thunk, add!::Function)\n\nA wrapper for a Thunk, that allows it to define an inplace add! function.\n\nadd! should be defined such that: ithunk.add!(Δ) = Δ .+= ithunk.val but it should do this more efficently than simply doing this directly. (Otherwise one can just use a normal Thunk).\n\nMost operations on an InplaceableThunk treat it just like a normal Thunk; and destroy its inplacability.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Thunk","page":"API","title":"ChainRulesCore.Thunk","text":"Thunk(()->v)\n\nA thunk is a deferred computation. It wraps a zero argument closure that when invoked returns a differential. @thunk(v) is a macro that expands into Thunk(()->v).\n\nCalling a thunk, calls the wrapped closure. If you are unsure if you have a Thunk, call unthunk which is a no-op when the argument is not a Thunk. If you need to unthunk recursively, call extern, which also externs the differial that the closure returns.\n\njulia> t = @thunk(@thunk(3))\nThunk(var\"#4#6\"())\n\njulia> extern(t)\n3\n\njulia> t()\nThunk(var\"#5#7\"())\n\njulia> t()()\n3\n\nWhen to @thunk?\n\nWhen writing rrules (and to a lesser exent frules), it is important to @thunk appropriately. Propagation rules that return multiple derivatives may not have all deriviatives used.  By @thunking the work required for each derivative, they then compute only what is needed.\n\nHow do thunks prevent work?\n\nIf we have res = pullback(...) = @thunk(f(x)), @thunk(g(x)) then if we did dx + res[1] then only f(x) would be evaluated, not g(x). Also if we did Zero() * res[1] then the result would be Zero() and f(x) would not be evaluated.\n\nSo why not thunk everything?\n\n@thunk creates a closure over the expression, which (effectively) creates a struct with a field for each variable used in the expression, and call overloaded.\n\nDo not use @thunk if this would be equal or more work than actually evaluating the expression itself. This is commonly the case for scalar operators.\n\nFor more details see the manual section on using thunks effectively\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.unthunk-Tuple{Any}","page":"API","title":"ChainRulesCore.unthunk","text":"unthunk(x)\n\nOn AbstractThunks this removes 1 layer of thunking. On any other type, it is the identity operation.\n\nIn contrast to extern this is nonrecursive.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.@thunk-Tuple{Any}","page":"API","title":"ChainRulesCore.@thunk","text":"@thunk expr\n\nDefine a Thunk wrapping the expr, to lazily defer its evaluation.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.extern-Tuple{Any}","page":"API","title":"ChainRulesCore.extern","text":"extern(x)\n\nMakes a best effort attempt to convert a differential into a primal value. This is not always a well-defined operation. For two reasons:\n\nIt may not be possible to determine the primal type for a given differential.\n\nFor example, Zero is a valid differential for any primal.\n\nThe primal type might not be a vector space, thus might not be a valid differential type.\n\nFor example, if the primal type is DateTime, it's not a valid differential type as two  DateTime can not be added (fun fact: Milisecond is a differential for DateTime).\n\nWhere it is defined the operation of extern for a primal type P should be extern(x) = zero(P) + x.\n\nnote: Note\nBecause of its limitations, extern should only really be used for testing. It can be useful, if you know what you are getting out, as it recursively removes thunks, and otherwise makes outputs more consistent with finite differencing.The more useful action in general is to call +, or in the case of a Thunk to call unthunk.\n\nwarning: Warning\nextern may return an alias (not necessarily a copy) to data wrapped by x, such that mutating extern(x) might mutate x itself.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Accumulation","page":"API","title":"Accumulation","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\"accumulation.jl\"]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.add!!-Tuple{Any,Any}","page":"API","title":"ChainRulesCore.add!!","text":"add!!(x, y)\n\nReturns x+y, potentially mutating x in-place to hold this value. This avoids allocations when x can be mutated in this way.\n\nSee also: InplaceableThunk.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Ruleset-Loading","page":"API","title":"Ruleset Loading","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\"ruleset_loading.jl\"]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.on_new_rule-Tuple{Any,Any}","page":"API","title":"ChainRulesCore.on_new_rule","text":"on_new_rule(hook, frule | rrule)\n\nRegister a hook function to run when new rules are defined. The hook receives a signature type-type as input, and generally will use eval to define an overload of an AD system's overloaded type For example, using the signature type Tuple{typeof(+), Real, Real} to make +(::DualNumber, ::DualNumber) call the frule for +. A signature type tuple always has the form: Tuple{typeof(operation), typeof{pos_arg1}, typeof{pos_arg2}...}, where pos_arg1 is the first positional argument.\n\nThe hooks are automatically run on new rules whenever a package is loaded. They can be manually triggered by refresh_rules. When a hook is first registered with on_new_rule it is run on all existing rules.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.refresh_rules-Tuple{}","page":"API","title":"ChainRulesCore.refresh_rules","text":"refresh_rules()\nrefresh_rules(frule | rrule)\n\nThis triggers all on_new_rule hooks to run on any newly defined rules. It is automatically run when ever a package is loaded. It can also be manually called to run it directly, for example if a rule was defined in the REPL or within the same file as the AD function.\n\n\n\n\n\n","category":"method"},{"location":"api.html#Internal","page":"API","title":"Internal","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"ChainRulesCore.AbstractDifferential\nChainRulesCore.debug_mode\nChainRulesCore.clear_new_rule_hooks!","category":"page"},{"location":"api.html#ChainRulesCore.AbstractDifferential","page":"API","title":"ChainRulesCore.AbstractDifferential","text":"The subtypes of AbstractDifferential define a custom \"algebra\" for chain rule evaluation that attempts to factor various features like complex derivative support, broadcast fusion, zero-elision, etc. into nicely separated parts.\n\nIn general a differential type is the type of a derivative of a value. The type of the value is for contrast called the primal type. Differential types correspond to primal types, although the relation is not one-to-one. Subtypes of  AbstractDifferential are not the only differential types. In fact for the most common primal types, such as Real or AbstractArray{Real} the the differential type is the same as the primal type.\n\nIn a circular definition: the most important property of a differential is that it should be able to be added (by defining +) to another differential of the same primal type. That allows for gradients to be accumulated.\n\nIt generally also should be able to be added to a primal to give back another primal, as this facilitates gradient descent.\n\nAll subtypes of AbstractDifferential implement the following operations:\n\n+(a, b): linearly combine differential a and differential b\n*(a, b): multiply the differential b by the scaling factor a\nBase.zero(x) = Zero(): a zero.\n\nFurther, they often implement other linear operators, such as conj, adjoint, dot. Pullbacks/pushforwards are linear operators, and their inputs are often AbstractDifferential subtypes. Pullbacks/pushforwards in-turn call other linear operators on those inputs. Thus it is desirable to have all common linear operators work on AbstractDifferentials.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.debug_mode","page":"API","title":"ChainRulesCore.debug_mode","text":"debug_mode() -> Bool\n\nDetermines if ChainRulesCore is in debug_mode. Defaults to false, but if the user redefines it to return true then extra information will be shown when errors occur.\n\nEnable via:\n\nChainRulesCore.debug_mode() = true\n\n\n\n\n\n","category":"function"},{"location":"api.html#ChainRulesCore.clear_new_rule_hooks!","page":"API","title":"ChainRulesCore.clear_new_rule_hooks!","text":"clear_new_rule_hooks!(frule|rrule)\n\nClears all hooks that were registered with corresponding on_new_rule. This is useful for while working interactively to define your rule generating hooks. If you previously wrong an incorrect hook, you can use this to get rid of the old one.\n\nwarning: Warning\nThis absolutely should not be used in a package, as it will break any other AD system using the rule hooks that might happen to be loaded.\n\n\n\n\n\n","category":"function"},{"location":"autodiff/operator_overloading.html#Operator-Overloading","page":"Operator Overloading","title":"Operator Overloading","text":"","category":"section"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"The principal interface for using the operator overload generation method is on_new_rule. This function allows one to register a hook to be run every time a new rule is defined. The hook receives a signature type-type as input, and generally will use eval to define an overload of an AD system's overloaded type. For example, using the signature type Tuple{typeof(+), Real, Real} to make  +(::DualNumber, ::DualNumber) call the frule for +. A signature type tuple always has the form: Tuple{typeof(operation), typeof{pos_arg1}, typeof{pos_arg2}, ...}, where pos_arg1 is the first positional argument. One can dispatch on the signature type to make rules with argument types your AD does not support not call eval; or more simply you can just use conditions for this. For example if your AD only supports AbstractMatrix{Float64} and Float64 inputs you might write:","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"const ACCEPT_TYPE = Union{Float64, AbstractMatrix{Float64}} \nfunction define_overload(sig::Type{<:Tuple{F, Vararg{ACCEPT_TYPE}}) where F\n    @eval quote\n        # ...\n    end\nend\ndefine_overload(::Any) = nothing  # don't do anything for any other signature\n\non_new_rule(define_overload, frule)","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"or you might write:","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"const ACCEPT_TYPES = (Float64, AbstractMatrix{Float64})\nfunction define_overload(sig)\n    sig = Base.unwrap_unionall(sig)  # not really handling most UnionAll,\n    opT, argTs = Iterators.peel(sig.parameters)\n    all(any(acceptT<: argT for acceptT in ACCEPT_TYPES) for argT in argTs) || return\n    @eval quote\n        # ...\n    end\nend\n\non_new_rule(define_overload, frule)","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"The generation of overloaded code is the responsibility of the AD implementor. Packages like ExprTools.jl can be helpful for this. Its generally fairly simple, though can become complex if you need to handle complicated type-constraints. Examples are shown below.","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"The hook is automatically triggered whenever a package is loaded. It can also be triggers manually using refresh_rules(@ref). This is useful for example if new rules are define in the REPL, or if a package defining rules is modified. (Revise.jl will not automatically trigger). When the rules are refreshed (automatically or manually), the hooks are only triggered on new/modified rules; not ones that have already had the hooks triggered on.","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"clear_new_rule_hooks!(@ref) clears all registered hooks. It is useful to undo [on_new_rule] hook registration if you are iteratively developing your overload generation function.","category":"page"},{"location":"autodiff/operator_overloading.html#Examples","page":"Operator Overloading","title":"Examples","text":"","category":"section"},{"location":"autodiff/operator_overloading.html#ForwardDiffZero","page":"Operator Overloading","title":"ForwardDiffZero","text":"","category":"section"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"The overload generation hook in this example is: define_dual_overload.","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"using Markdown\nMarkdown.parse(\"\"\"\n```julia\n$(read(joinpath(@__DIR__,\"../../../test/demos/forwarddiffzero.jl\"), String))\n```\n\"\"\")","category":"page"},{"location":"autodiff/operator_overloading.html#ReverseDiffZero","page":"Operator Overloading","title":"ReverseDiffZero","text":"","category":"section"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"The overload generation hook in this example is: define_tracked_overload.","category":"page"},{"location":"autodiff/operator_overloading.html","page":"Operator Overloading","title":"Operator Overloading","text":"using Markdown\nMarkdown.parse(\"\"\"\n```julia\n$(read(joinpath(@__DIR__,\"../../../test/demos/reversediffzero.jl\"), String))\n```\n\"\"\")","category":"page"},{"location":"design/many_differentials.html#Design-Notes:-The-many-to-many-relationship-between-differential-types-and-primal-types.","page":"Many Differential Types","title":"Design Notes: The many-to-many relationship between differential types and primal types.","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"ChainRules has a system where one primal type (the type having its derivative taken) can have multiple possible differential types (the type of the derivative); and where one differential type can correspond to multiple primal types. This is in-contrast to the Swift AD efforts, which has one differential type per primal type (Swift uses the term associated tangent type, rather than differential type).","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"terminology: differential and associated tangent type\nThe use of “associated tangent type” in AD is not technically correct, as differentials naturally live in the cotangent plane instead of the tangent plane. However it is often reasonable for AD to treat the cotangent plane and tangent plane as the same thing, and this was an intentional choice by the Swift team. Here we will just stick to the ChainRules terminology and only say “differential type” instead of “tangent type”.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"One thing to understand about differentials is that they have to form a vector space  (or something very like them). They need to support addition to each other, they need a zero which doesn't change what it is added to, and they need to support scalar multiplication (this isn't really required, but it is handy for things like gradient descent). Beyond being a vector space, differentials need to be able to be added to a primal value to get back another primal value. Or roughly equivalently a differential is a difference between two primal values.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"One thing to note in this example is that the primal does not have to be a vector. As an example, consider DateTime. A DateTime is not a vector space: there is no origin point, and DateTimes cannot be added to each other. The corresponding differential type is any subtype of Period, such as Millisecond, Hour, Day etc.","category":"page"},{"location":"design/many_differentials.html#Natural-differential","page":"Many Differential Types","title":"Natural differential","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"For a given primal type, we say a natural differential type is one which people would intuitively think of as representing the difference between two primal values. It tends to already exist outside of the context of AD. So Millisecond, Hour, Day etc. are examples of natural differentials for the DateTime primal.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Note here that we already have a one primal type to many differential types relationship. We have Millisecond and Hour and Day all being valid differential types for DateTime. In this case we could convert them all to a single differential type, such as Nanoseconds, but that is not always a reasonable decision: we may run in to overflow, or lots of allocations if we need to use a BigInt to represent the number of Nanosecond since the start of the universe. For types with more complex semantics, such as array types, these considerations are much more important.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Natural differential types are the types people tend to think in, and thus the type they tend to write custom sensitivity rules in. An important special case of natural differentials is when the primal type is a vector space (e.g. Real,AbstractMatrix) in which case it is common for the natural differential type to be the same as the primal type. One exception to this is getindex. The ideal choice of differential type for getindex on a dense array would be some type of sparse array, due to the fact the derivative will have only one non-zero element. This actually further brings us to a weirdness of differential types not actually being closed under addition, as it would be ideal for the sparse array to become a dense array if summed over all elements.","category":"page"},{"location":"design/many_differentials.html#Structural-differential-types","page":"Many Differential Types","title":"Structural differential types","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"AD cannot automatically determine natural differential types for a primal. For some types we may be able to declare manually their natural differential type. Other types will not have natural differential types at all - e.g. NamedTuple, Tuple, WebServer, Flux.Dense -  so we are destined to make some up. So beyond natural differential types, we also have structural differential types. ChainRules uses Composite{P, <:NamedTuple} to represent a structural differential type corresponding to primal type P. Zygote v0.4 uses NamedTuple.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Structural differentials are derived from the structure of the input. Either automatically, as part of the AD, or manually, as part of a custom rule.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Consider the structure of DateTime:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"julia> dump(now())\nDateTime\n  instant: UTInstant{Millisecond}\n    periods: Millisecond\n      value: Int64 63719890305605","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"The corresponding structural differential is:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Composite{DateTime}(\n    instant::Composite{UTInstant{Millisecond}}(\n        periods::Composite{Millisecond}(\n            value::Int64\n        )\n    )\n)","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"note: One must be allowed to take derivatives of integer arguments\nThis brings up another contrast to Swift. In Swift Int is considered non-differentiable, which is quite reasonable; it doesn’t have a very good definition of the limit of a small step (as that would be some floating/fixed point type). Int is intrinsically discrete. It is commonly used for indexing, and if one takes a gradient step, say turning x[2] into x[2.1] then that is an error. However, disallowing Int to be used as a differential means we cannot handle cases like DateTime having an inner field of milliseconds counted as an integer from the unix epoch or other cases where an integer is used as a convenience for computational efficiency. In the case where a custom sensitivity rule claims that there is a non-zero derivative for an Int argument that is being used for indexing, that code is simply wrong. We can’t handle incorrect code and trying to is a path toward madness. Julia, unlike Swift, is not well suited to handling rules about what you can and can’t do with particular types.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"So the structural differential is another type of differential. We must support both natural and structural differentials because AD can only create structural differentials (unless using custom sensitivity rules) and all custom sensitivities are only written in terms of natural differentials, as that is what is used in papers about derivatives.","category":"page"},{"location":"design/many_differentials.html#Semi-structural-differentials","page":"Many Differential Types","title":"Semi-structural differentials","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Where there is no natural differential type for the outermost type but there is for some of its fields, we call this a \"semi-structural\" differential.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Consider if we had a representation of a country's GDP as output by some continuous time model like a Gaussian Process, where that representation is as a sequence of TimeSamples structured as follows:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"julia> struct TimeSample\n           time::DateTime\n           value::Float64\n       end","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"We can look at its structure:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"julia> dump(TimeSample(now(), 2.6e9))\nTimeSample\n  time: DateTime\n    instant: Dates.UTInstant{Millisecond}\n      periods: Millisecond\n        value: Int64 63720043490844\n  value: Float64 2.6e9","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Thus we see the that structural differential would be:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Composite{TimeSample}(\n    time::Composite{DateTime}(\n        instant::Composite{UTInstant{Millisecond}}(\n            periods::Composite{Millisecond}(\n                value::Int64\n            )\n        )\n    ),\n    value::Float64\n)","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"But instead in the custom sensitivity rule we would write a semi-structured differential type. Since there is not a natural differential type for TimeSample but there is for DateTime.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Composite{TimeSample}(\n    time::Day,\n    value::Float64\n)","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"So the rule author has written a structural differential with some fields that are natural differentials.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Another related case is for types that overload getproperty such as SVD and QR. In this case the structural differential will be based on the fields, but those fields do not always have an easy relation to what is actually used in math. For example, the QR type has fields factors and t, but we would more naturally think in terms of the properties Q and R. So most rule authors would want to write semi-structural differentials based on the properties.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"To return to the question of why ChainRules has Composite{P, <:NamedTuple} whereas Zygote v0.4 just has NamedTuple, it relates to semi-structural derivatives, and being able to overload things more generally. If one knows that one has a semi-structural derivative based on property names, like Composite{QR}(Q=..., R=...), and one is adding it to the true structural derivative based on field names Composite{QR}(factors=..., τ=...), then we need to overload the addition operator to perform that correctly. We cannot happily overload similar things for NamedTuple since we don't know the primal type, only the names of the values contained. In fact we can't actually overload addition at all for NamedTuple as that would be type-piracy, so have to use Zygote.accum instead.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Another use of the primal being a type parameter is to catch errors. ChainRules disallows the addition of Composite{SVD} to Composite{QR} since in a correctly differentiated program that can never occur.","category":"page"},{"location":"design/many_differentials.html#Differentials-types-for-computational-efficiency","page":"Many Differential Types","title":"Differentials types for computational efficiency","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"There is another kind of unnatural differential. One that is for computational efficiency. ChainRules has Thunks and InplaceableThunks, which wrap the computation of a derivative and delays that work until it is needed, either via the derivative being added to something or being unthunked manually, thus saving time if it is never used.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Another differential type used for efficiency is Zero which represents the hard zero (in Zygote v0.4 this is nothing). For example the derivative of f(x, y)=2x with respect to y is Zero(). Add Zero() to anything, and one gets back the original thing without change. We noted that all differentials need to be a vector space.  Zero() is the trivial vector space. Further, add Zero() to any primal value (no matter the type) and you get back another value of the same primal type (the same value in fact). So it meets the requirements of a differential type for all primal types. Zero can save on memory (since we can avoid allocating anything) and on time (since performing the multiplication Zero and Thunk are both examples of a differential type that is valid for multiple primal types.","category":"page"},{"location":"design/many_differentials.html#Conclusion","page":"Many Differential Types","title":"Conclusion","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Now, you have seen examples of both differential types that work for multiple primal types, and primal types that have  multiple valid differential types. Semantically we can handle these very easily in julia. Just put in a few more dispatching on +. Multiple-dispatch is great like that. The down-side is our type-inference becomes hard. If you have exactly 1 differential type for each primal type, you can very easily workout what all the types on your reverse pass will be - you don't really need type inference - but you lose so much expressibility.","category":"page"},{"location":"design/many_differentials.html#Appendix:-What-Swift-does","page":"Many Differential Types","title":"Appendix: What Swift does","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"I don't know how Swift is handling thunks, maybe they are not, maybe they have an optimizing compiler that can just slice out code-paths that don't lead to values that get used; maybe they have a language built in for lazy computation.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"They are, as I understand it, handling Zero by requiring every differential type to define a zero method – which it has since it is a vector space. This costs memory and time, but probably not actually all that much. With regards to handling multiple different differential types for one primal, like natural and structural derivatives, everything needs to be converted to the canonical differential type of that primal.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"As I understand it, things can be automatically converted by defining conversion protocols or something like that, so rule authors can return anything that has a conversion protocol to the canonical differential type of the primal.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"However, it seems like this will run into problems. Recall that the natural differential in the case of getindex on an AbstractArray was a sparse array. But for say the standard dense Array, the only reasonable canonical differential type is also a dense Array. But if you convert a sparse array into a dense array you do giant allocations to fill in all the other entries with zero.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"So this is the story about why we have many-to-many differential types in ChainRules.","category":"page"},{"location":"writing_good_rules.html#On-writing-good-rrule-/-frule-methods","page":"Writing Good Rules","title":"On writing good rrule / frule methods","text":"","category":"section"},{"location":"writing_good_rules.html#Use-Zero()-or-One()-as-return-value","page":"Writing Good Rules","title":"Use Zero() or One() as return value","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"The Zero() and One() differential objects exist as an alternative to directly returning 0 or zeros(n), and 1 or I. They allow more optimal computation when chaining pullbacks/pushforwards, to avoid work. They should be used where possible.","category":"page"},{"location":"writing_good_rules.html#Use-Thunks-appropriately","page":"Writing Good Rules","title":"Use Thunks appropriately","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"If work is only required for one of the returned differentials, then it should be wrapped in a @thunk (potentially using a begin-end block).","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"If there are multiple return values, their computation should almost always be wrapped in a @thunk.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Do not wrap variables in a @thunk; wrap the computations that fill those variables in @thunk:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"# good:\n∂A = @thunk(foo(x))\nreturn ∂A\n\n# bad:\n∂A = foo(x)\nreturn @thunk(∂A)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"In the bad example foo(x) gets computed eagerly, and all that the thunk is doing is wrapping the already calculated result in a function that returns it.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Do not use @thunk if this would be equal or more work than actually evaluating the expression itself. Examples being:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"The expression being a constant\nThe expression is merely wrapping something in a struct, such as Adjoint(x) or Diagonal(x)\nThe expression being itself a thunk\nThe expression being from another rrule or frule; it would be @thunked if required by the defining rule already.\nThere is only one derivative being returned, so from the fact that the user called frule/rrule they clearly will want to use that one.","category":"page"},{"location":"writing_good_rules.html#Code-Style","page":"Writing Good Rules","title":"Code Style","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Use named local functions for the pullback in an rrule.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"# good:\nfunction rrule(::typeof(foo), x)\n    Y = foo(x)\n    function foo_pullback(x̄)\n        return NO_FIELDS, bar(x̄)\n    end\n    return Y, foo_pullback\nend\n#== output\njulia> rrule(foo, 2)\n(4, var\"#foo_pullback#11\"())\n==#\n\n# bad:\nfunction rrule(::typeof(foo), x)\n    return foo(x), x̄ -> (NO_FIELDS, bar(x̄))\nend\n#== output:\njulia> rrule(foo, 2)\n(4, var\"##9#10\"())\n==#","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"While this is more verbose, it ensures that if an error is thrown during the pullback the gensym name of the local function will include the name you gave it. This makes it a lot simpler to debug from the stacktrace.","category":"page"},{"location":"writing_good_rules.html#Write-tests","page":"Writing Good Rules","title":"Write tests","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"In ChainRulesTestUtils.jl there are fairly decent tools for writing tests based on FiniteDifferences.jl. Take a look at existing ChainRules.jl tests and you should see how to do stuff.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"warning: Warning\nUse finite differencing to test derivatives. Don't use analytical derivations for derivatives in the tests. Those are what you use to define the rules, and so can not be confidently used in the test. If you misread/misunderstood them, then your tests/implementation will have the same mistake.","category":"page"},{"location":"writing_good_rules.html#CAS-systems-are-your-friends.","page":"Writing Good Rules","title":"CAS systems are your friends.","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"It is very easy to check gradients or derivatives with a computer algebra system (CAS) like WolframAlpha.","category":"page"},{"location":"FAQ.html#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"FAQ.html#What-is-up-with-the-different-symbols?","page":"FAQ","title":"What is up with the different symbols?","text":"","category":"section"},{"location":"FAQ.html#Δx,-x,-dx","page":"FAQ","title":"Δx, ∂x, dx","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ChainRules uses these perhaps atypically. As a notation that is the same across propagators, regardless of direction (incontrast see ẋ and x̄ below).","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Δx is the input to a propagator, (i.e a seed for a pullback; or a perturbation for a pushforward)\n∂x is the output of a propagator\ndx could be either input or output","category":"page"},{"location":"FAQ.html#dots-and-bars:-\\dot{y}-\\dfrac{y}{x}-\\overline{x}","page":"FAQ","title":"dots and bars: doty = dfracyx = overlinex","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"v̇ is a derivative of the input moving forward: v = fracvx for input x, intermediate value v.\nv̄ is a derivative of the output moving backward: v = fracyv for output y, intermediate value v.","category":"page"},{"location":"FAQ.html#others","page":"FAQ","title":"others","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Ω is often used as the return value of the function. Especially, but not exclusively, for scalar functions.\nΔΩ is thus a seed for the pullback.\n∂Ω is thus the output of a pushforward.","category":"page"},{"location":"FAQ.html#Why-does-rrule-return-the-primal-function-evaluation?","page":"FAQ","title":"Why does rrule return the primal function evaluation?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"You might wonder why frule(f, x) returns f(x) and the derivative of f at x, and similarly for rrule returning f(x) and the pullback for f at x. Why not just return the pushforward/pullback, and let the user call f(x) to get the answer separately?","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"There are three reasons the rules also calculate the f(x).","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"For some rules an alternative way of calculating f(x) can give the same answer while also generating intermediate values that can be used in the calculations required to propagate the derivative.\nFor many rrules the output value is used in the definition of the pullback. For example tan, sigmoid etc.\nFor some frules there exists a single, non-separable operation that will compute both derivative and primal result. For example many of the methods for differential equation sensitivity analysis.","category":"page"},{"location":"FAQ.html#Where-are-the-derivatives-for-keyword-arguments?","page":"FAQ","title":"Where are the derivatives for keyword arguments?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"pullbacks do not return a sensitivity for keyword arguments; similarly pushfowards do not accept a perturbation for keyword arguments. This is because in practice functions are very rarely differentiable with respect to keyword arguments. As a rule keyword arguments tend to control side-effects, like logging verbosity, or to be functionality changing to perform a different operation, e.g. dims=3, and thus not differentiable. To the best of our knowledge no Julia AD system, with support for the definition of custom primitives, supports differentiating with respect to keyword arguments. At some point in the future ChainRules may support these. Maybe.","category":"page"},{"location":"FAQ.html#What-is-the-difference-between-Zero-and-DoesNotExist-?","page":"FAQ","title":"What is the difference between Zero and DoesNotExist ?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Zero and DoesNotExist act almost exactly the same in practice: they result in no change whenever added to anything. Odds are if you write a rule that returns the wrong one everything will just work fine. We provide both to allow for clearer writing of rules, and easier debugging.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Zero() represents the fact that if one perturbs (adds a small change to) the matching primal there will be no change in the behavour of the primal function. For example in fst(x,y) = x, then the derivative of fst with respect to y is Zero(). fst(10, 5) == 10 and if we add 0.1 to 5 we still get fst(10, 5.1)=10.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"DoesNotExist() represents the fact that if one perturbs the matching primal, the primal function will now error. For example in access(xs, n) = xs[n] then the derivative of access with respect to n is DoesNotExist(). access([10, 20, 30], 2) = 20, but if we add 0.1 to 2 we get access([10, 20, 30], 2.1) which errors as indexing can't be applied at fractional indexes.","category":"page"},{"location":"FAQ.html#When-to-use-ChainRules-vs-ChainRulesCore?","page":"FAQ","title":"When to use ChainRules vs ChainRulesCore?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ChainRulesCore.jl is a light-weight dependency for defining rules for functions in your packages, without you needing to depend on ChainRules.jl itself. It has almost no dependencies of its own. If you only want to define rules, not use them, then you probably only want to load ChainRulesCore.jl.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ChainRules.jl provides the full functionality for AD systems, in particular it has all the rules for Base Julia and the standard libraries. It is thus a much heavier package to load. AD systems making use of frules and rrules should load ChainRules.jl.","category":"page"},{"location":"FAQ.html#Where-should-I-put-my-rules?","page":"FAQ","title":"Where should I put my rules?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"We recommend adding custom rules to your own packages with ChainRulesCore.jl, rather than adding them to ChainRules.jl. A few packages - currently SpecialFunctions.jl and NaNMath.jl - have rules in ChainRules.jl as a short-term measure.","category":"page"},{"location":"FAQ.html#How-do-I-test-my-rules?","page":"FAQ","title":"How do I test my rules?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"You can use ChainRulesTestUtils.jl to test your custom rules. ChainRulesTestUtils.jl has some dependencies, so it is a separate package from ChainRulesCore.jl. This means your package can depend on the light-weight ChainRulesCore.jl, and make ChainRulesTestUtils.jl a test-only dependency.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Remember to read the section on On writing good rrule / frule methods.","category":"page"},{"location":"FAQ.html#Where-can-I-learn-more-about-AD-?","page":"FAQ","title":"Where can I learn more about AD ?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"There are not so many truly excellent learning resources for autodiff out there in the world, which is a bit sad. The list here is incomplete, but is vetted for quality.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Automatic Differentiation for Dummies keynote video by Simon Peyton Jones: particularly good if you like pure math type thinking.\nMIT 18337 lecture notes 8-10 (by Christopher Rackauckas and David P. Sanders  : moves fast from basic to advanced, particularly good if you like applicable mathematics\nAutomatic Differentiation and Application: Good introduction\nForward-Mode AD via High Dimensional Algebras: actually part 2 of the introduction\nSolving Stiff Ordinary Differential Equations: ignore the ODE stuff, most of this is about Sparse AutoDiff, can skip/skim this one\nBasic Parameter Estimation, Reverse-Mode AD, and Inverse Problems: use in optimization, and details connections of other math.\nDiff-Zoo Jupyter Notebook Book  (by Mike Innes, has implementations and explanations.\n\"Evaluating Derivatives\" (by Griewank and Walther) is the best book at least for reverse-mode.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"It also covers forward-mode though (by its own admission) not as well, it never mentioned dual numbers which is an unfortunate lack.","category":"page"},{"location":"autodiff/overview.html#Using-ChainRules-in-your-AD-system","page":"Overview","title":"Using ChainRules in your AD system","text":"","category":"section"},{"location":"autodiff/overview.html","page":"Overview","title":"Overview","text":"This section is for authors of AD systems. It assumes a pretty solid understanding of both Julia and automatic differentiation. It explains how to make use of ChainRule's \"rulesets\" (frules, rrules,) to avoid having to code all your own AD primitives / custom sensitives.","category":"page"},{"location":"autodiff/overview.html","page":"Overview","title":"Overview","text":"There are 3 main ways to access ChainRules rule sets in your AutoDiff system.","category":"page"},{"location":"autodiff/overview.html","page":"Overview","title":"Overview","text":"Operation Overloading Generation\nThis is primarily intended for operator overloading based AD systems which will generate overloads for primal functions based for their overloaded types based on the existence of an rrule/frule.\nA source code generation based AD can also use this by overloading their transform generating function directly so as not to recursively generate a transform but to just return the rule.\nThis does not play nice with Revise.jl, adding or modifying rules in loaded files will not be reflected until a manual refresh, and deleting rules will not be reflected at all.\nSource code tranform based on inserting branches that check of rrule/frule return nothing\nIf the rrule/frule returns a rule result then use it, if it returns nothing then do normal AD path.\nIn theory type inference optimizes these branchs out; in practice it may not.\nThis is a fairly simple Cassette overdub (or similar) of all calls, and is suitable for overloading based AD or source code transformation.\nSource code transform based on rrule/frule method-table\nIf an applicable rrule/frule exists in the method table then use it, else generate normal AD path.\nThis avoids having branches in your generated code.\nThis requires maintaining your own back-edges.\nThis is pretty hardcore even by the standard of source code tranformations.","category":"page"},{"location":"debug_mode.html#Debug-Mode","page":"Debug Mode","title":"Debug Mode","text":"","category":"section"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"ChainRulesCore supports a debug mode which you can use while writing new rules. It provides better error messages. If you are developing some new rules, and you get a weird error message, it is worth enabling debug mode.","category":"page"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"There is some overhead to having it enabled, so it is disabled by default.","category":"page"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"To enable, redefine the ChainRulesCore.debug_mode function to return true.","category":"page"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"ChainRulesCore.debug_mode() = true","category":"page"},{"location":"arrays.html#Deriving-Array-Rules","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"One of the goals of the ChainRules interface is to make it easy to define your own rules for a function. This tutorial attempts to demystify deriving and implementing custom rules for arrays with real and complex entries, with examples. The approach we use is similar to the one succinctly explained and demonstrated in [Giles2008] and its extended work [Giles2008ext], but we generalize it to support functions of multidimensional arrays with both real and complex entries.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Throughout this tutorial, we will use the following type alias:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"const RealOrComplex = Union{Real,Complex}","category":"page"},{"location":"arrays.html#Forward-mode-rules","page":"Deriving Array Rules","title":"Forward-mode rules","text":"","category":"section"},{"location":"arrays.html#Approach","page":"Deriving Array Rules","title":"Approach","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Consider a function","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = f(X::Array{<:RealOrComplex}...)::Array{<:RealOrComplex}","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"or in math notation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"f (ldots X_m ldots) mapsto Omega","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where the components of X_m are written as (X_m)_ildotsj. The variables X_m and Omega are intermediates in a larger program (function) that, by considering only a single real input t and real output s can always be written as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"t mapsto (ldots X_m ldots) mapsto Omega mapsto s","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where t and s are real numbers. If we know the partial derivatives of X_m with respect to t, fracdX_mdt = dotX_m, the chain rule gives the pushforward of f as:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation labelpf\ndotOmega\n    = f_*(ldots dotX_m ldots)\n    = sum_m sum_i ldots j\n        fracpartial Omega partial (X_m)_ildotsj  (dotX_m)_ildotsj\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"That's ugly, but in practice we can often write it more simply by using forward mode rules for simpler functions, as we'll see below. The forward-mode rules for arrays follow directly from the usual scalar chain rules.","category":"page"},{"location":"arrays.html#Array-addition","page":"Deriving Array Rules","title":"Array addition","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = A + B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"This one is easy:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega = A + B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega = dotA + dotB","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can implement the frule in ChainRules's notation:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule(\n    (_, ΔA, ΔB),\n    ::typeof(+),\n    A::Array{<:RealOrComplex},\n    B::Array{<:RealOrComplex},\n)\n    Ω = A + B\n    ∂Ω = ΔA + ΔB\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Matrix-multiplication","page":"Deriving Array Rules","title":"Matrix multiplication","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = A * B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega = A B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"First we write in component form:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega_ij = sum_k A_ik B_kj","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Then we use the product rule to get the pushforward for each scalar entry:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\ndotOmega_ij\n    = sum_k left( dotA_ik B_kj + A_ik dotB_kj right)\n         textapply scalar product rule \n            fracddt(x y) = fracdxdt y + x fracdydt \n    = sum_k dotA_ik B_kj + sum_k A_ik dotB_kj\n         textsplit sum\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"But the last expression is just the component form of a sum of matrix products:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequationlabeldiffprod\ndotOmega = dotA B + A dotB\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"This is the matrix product rule, and we write its frule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule(\n    (_, ΔA, ΔB),\n    ::typeof(*),\n    A::Matrix{<:RealOrComplex},\n    B::Matrix{<:RealOrComplex},\n)\n    Ω = A * B\n    ∂Ω = ΔA * B + A * ΔB\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Matrix-inversion","page":"Deriving Array Rules","title":"Matrix inversion","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = inv(A)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega = A^-1","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"It's easiest to derive this rule from either of the two constraints:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nOmega A = A^-1 A = I\nA Omega = A A^-1 = I\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where I is the identity matrix.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We use the matrix product rule to differentiate the first constraint:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega A + Omega dotA = 0","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Then, right-multiply both sides by A^-1 to isolate dotOmega:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\n0  = dotOmega A A^-1 + Omega dotA A^-1 nonumber\n   = dotOmega I + Omega dotA A^-1\n        textuse  A A^-1 = I nonumber\n   = dotOmega + Omega dotA Omega\n        textsubstitute  A^-1 = Omega nonumber\ndotOmega\n   = -Omega dotA Omega\n        textsolve for  dotOmega labelinvdiff\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We write the frule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule((_, ΔA), ::typeof(inv), A::Matrix{<:RealOrComplex})\n    Ω = inv(A)\n    ∂Ω = -Ω * ΔA * Ω\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Other-useful-identities","page":"Deriving Array Rules","title":"Other useful identities","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"These identities are particularly useful:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nfracddt left( operatornamereal(A) right) = operatornamereal(dotA)\nfracddt left( operatornameconj(A) right) = operatornameconj(dotA)\nfracddt left( A^T right) = dotA^T\nfracddt left( A^H right) = dotA^H\nfracddt left( sum_j  A_i ldots j ldots k right) =\n    sum_j dotA_i ldots j ldots k\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where cdot^H = operatornameconj(cdot^T) is the conjugate transpose (the adjoint function).","category":"page"},{"location":"arrays.html#Reverse-mode-rules","page":"Deriving Array Rules","title":"Reverse-mode rules","text":"","category":"section"},{"location":"arrays.html#Approach-2","page":"Deriving Array Rules","title":"Approach","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Reverse-mode rules are a little less intuitive, but we can re-use our pushforwards to simplify their derivation. Recall our program:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"t mapsto (ldots X_m ldots) mapsto Omega mapsto s","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"At any step in the program, if we have intermediates X_m, we can write down the derivative fracdsdt in terms of the tangents dotX_m = fracdX_mdt and adjoints overlineX_m = fracpartial spartial X_m","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nfracdsdt\n    = sum_m operatornamerealleft( sum_ildotsj\n           operatornameconjleft( fracpartial spartial (X_m)_ildotsj right)\n           fracd (X_m)_ildotsjdt\n       right)\n    = sum_m operatornamerealleft( sum_ildotsj\n           operatornameconj left( (overlineX_m)_ildotsj right)\n           (dotX_m)_ildotsj\n       right)\n    = sum_m operatornamerealleft( operatornamedotleft(\n           overlineX_m dotX_m\n       right) right)\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where operatornameconj(cdot) is the complex conjugate (conj), operatornamereal(cdot) is the real part of its argument (real), and operatornamedot(cdot cdot) is the inner product (LinearAlgebra.dot). Because this equation follows at any step of the program, we can equivalently write ","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"fracdsdt = operatornamerealleft( operatornamedotleft(\n                    overlineOmega dotOmega\n                right) right)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"which gives the identity","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation labelpbident\noperatornamerealleft( operatornamedotleft(\n    overlineOmega dotOmega\nright) right) = \nsum_m operatornamerealleft( operatornamedotleft(\n    overlineX_m dotX_m\nright) right)\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"For matrices and vectors, operatornamedot(A B) = operatornametr(A^H B), and the identity simplifies to:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation labelpbidentmat\noperatornamerealleft( operatornametrleft(\n    overlineOmega^H dotOmega\nright) right) =\nsum_m operatornamereal left( operatornametr left(\n    overlineX_m^H dotX_m\nright) right)\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where operatornametr(cdot) is the matrix trace (LinearAlgebra.tr) function.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Our approach for deriving the adjoints overlineX_m is then:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Derive the pushforward (dotOmega in terms of dotX_m) using \\eqref{pf}.\nSubstitute this expression for dotOmega into the left-hand side of \\eqref{pbident}.\nManipulate until it looks like the right-hand side of \\eqref{pbident}.\nSolve for each overlineX_m.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Note that the final expressions for the adjoints will not contain any dotX_m terms.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"note: Note\nWhy do we conjugate, and why do we only use the real part of the dot product in \\eqref{pbident}? Recall from Complex Numbers that we treat a complex number as a pair of real numbers. These identities are a direct consequence of this convention. Consider fracdsdt for a scalar function f (x + i y) mapsto (u + i v):beginalign*\nfracdsdt\n    = operatornamerealleft( operatornamedotleft(\n           overlinex + i overliney dotx + i doty\n       right) right) \n    = operatornamerealleft(\n           operatornameconj left( overlinex + i overliney right)\n           left( dotx + i doty right)\n       right) \n    = operatornamerealleft(\n           left( overlinex - i overliney right)\n           left( dotx + i doty right)\n       right) \n    = operatornamerealleft(\n           left( overlinex dotx + overliney doty right) +\n           i left( overlinex doty - overliney dotx right)\n       right)\n    = overlinex dotx + overliney doty\nendalign*which is exactly what the identity would produce if we had written the function as f (x y) mapsto (u v).","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"For matrices and vectors, several properties of the trace function come in handy:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\noperatornametr(A+B) = operatornametr(A) + operatornametr(B) labeltrexpand\noperatornametr(A^T) = operatornametr(A) nonumber\noperatornametr(A^H) = operatornameconj(operatornametr(A)) nonumber\noperatornametr(AB) = operatornametr(BA) labeltrperm\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Now let's derive a few pullbacks using this approach.","category":"page"},{"location":"arrays.html#Matrix-multiplication-2","page":"Deriving Array Rules","title":"Matrix multiplication","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = A * B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We above derived in \\eqref{diffprod} the pushforward","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega = dotA B + A dotB","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Using \\eqref{pbidentmat}, we now multiply by overlineOmega^H and take the real trace:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noperatornamerealleft( operatornametr left(\n        overlineOmega^H dotOmega\nright) right)\n    = operatornamerealleft( operatornametr left( overlineOmega^H left(\n           dotA B + A dotB\n       right) right) right)\n            textsubstitute  dotOmega text from  eqrefdiffprod\n    = operatornamerealleft( operatornametr left(\n           overlineOmega^H dotA B\n       right) right) +\n       operatornamerealleft( operatornametr left(\n           overlineOmega^H  A dotB\n       right) right)\n            textexpand using  eqreftrexpand \n    = operatornamerealleft( operatornametr left(\n           B overlineOmega^H dotA\n       right) right) +\n       operatornamerealleft( operatornametr left(\n           overlineOmega^H A dotB\n       right) right)\n            textrearrange the left term using  eqreftrperm\n    = operatornamerealleft( operatornametr left(\n           overlineA^H  dotA\n       right) right) +\n       operatornamerealleft( operatornametr left(\n           overlineB^H dotB\n       right) right)\n            textright-hand side of  eqrefpbidentmat\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"That's it! The expression is in the desired form to solve for the adjoints by comparing the last two lines:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nB overlineOmega^H dotA = overlineA^H  dotA quad\n     overlineA = overlineOmega B^H\noverlineOmega^H A dotB = overlineB^H dotB quad\n     overlineB = A^H overlineOmega\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Using ChainRules's notation, we would implement the rrule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(*), A::Matrix{<:RealOrComplex}, B::Matrix{<:RealOrComplex})\n    function times_pullback(ΔΩ)\n        ∂A = @thunk(ΔΩ * B')\n        ∂B = @thunk(A' * ΔΩ)\n        return (NO_FIELDS, ∂A, ∂B)\n    end\n    return A * B, times_pullback\nend","category":"page"},{"location":"arrays.html#Matrix-inversion-2","page":"Deriving Array Rules","title":"Matrix inversion","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = inv(A)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"In \\eqref{invdiff}, we derived the pushforward as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega = -Omega dotA Omega","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Using \\eqref{pbidentmat},","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noperatornamerealleft( operatornametr left(\n    overlineOmega^H dotOmega\nright) right)\n    = operatornamerealleft( operatornametr left(\n           -overlineOmega^H Omega dotA Omega\n       right) right)\n            textsubstitute  eqrefinvdiff\n    = operatornamerealleft( operatornametr left(\n           -Omega overlineOmega^H Omega dotA\n       right) right)\n            textrearrange using  eqreftrperm\n    = operatornamerealleft( operatornametr left(\n           overlineA^H  dotA\n       right) right)\n            textright-hand side of  eqrefpbidentmat\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"we can now solve for overlineA:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"overlineA = left( -Omega overlineOmega^H Omega right)^H\n             = -Omega^H overlineOmega Omega^H","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can implement the resulting rrule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(inv), A::Matrix{<:RealOrComplex})\n    Ω = inv(A)\n    function inv_pullback(ΔΩ)\n        ∂A = -Ω' * ΔΩ * Ω'\n        return (NO_FIELDS, ∂A)\n    end\n    return Ω, inv_pullback\nend","category":"page"},{"location":"arrays.html#A-multidimensional-array-example","page":"Deriving Array Rules","title":"A multidimensional array example","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We presented the approach for deriving pushforwards and pullbacks for arrays of arbitrary dimensions, so let's cover an example. For multidimensional arrays, it's often easier to work in component form. Consider the following function:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = sum(abs2, X::Array{<:RealOrComplex,3}; dims=2)::Array{<:Real,3}","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"which we write as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega_i1k = sum_j X_ijk^2\n             = sum_j operatornamereal left(\n                  operatornameconj left( X_ijk right) X_ijk\n               right)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The pushforward from \\eqref{pf} is","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\ndotOmega_i1k\n    = sum_j operatornamerealleft(\n           operatornameconj left( dotX_ijk right) X_ijk +\n           operatornameconj left( X_ijk right) dotX_ijk right) nonumber\n    = sum_j operatornamerealleft(\n            operatornameconjleft(\n                operatornameconj left( X_ijk right) dotX_ijk\n            right) +\n            operatornameconj(X_ijk) dotX_ijk\n       right) nonumber\n    = sum_j 2 operatornamerealleft(\n           operatornameconj left( X_ijk right) dotX_ijk\n       right) labelsumabspf\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where in the last step we have used the fact that for all real a and b,","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"(a + i b) + operatornameconj(a + i b)\n    = (a + i b) + (a - i b)\n    = 2 a\n    = 2 operatornamereal (a + i b)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Because none of this derivation depended on the index (or indices), we implement frule generically as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule(\n    (_, _, ΔX),\n    ::typeof(sum),\n    ::typeof(abs2),\n    X::Array{<:RealOrComplex};\n    dims = :,\n)\n    Ω = sum(abs2, X; dims = dims)\n    ∂Ω = sum(2 .* real.(conj.(X) .* ΔX); dims = dims)\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can now derive the reverse-mode rule. The array form of \\eqref{pbident} is","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noperatornamerealleft( operatornamedotleft(\n    overlineOmega dotOmega\nright) right)\n    = operatornamereal left( sum_ik\n           operatornameconj left( overlineOmega_i1k right) dotOmega_i1k\n       right)\n            textexpand left-hand side of  eqrefpbident\n    = operatornamereal left(sum_ijk\n           operatornameconj left( overlineOmega_i1k right)\n           2 operatornamerealleft(\n               operatornameconj left( X_ijk right) dotX_ijk\n           right)\n       right)\n            textsubstitute  eqrefsumabspf\n    = operatornamereal left( sum_ijk\n           left(\n               2 operatornamereal left( overlineOmega_i1k right)\n               operatornameconj left( X_ijk right)\n           right) dotX_ijk\n       right)\n            textbring  dotX_ijk text outside of  operatornamereal\n    = operatornamereal left( sum_ijk\n           operatornameconj left( overlineX_ijk right) dotX_i1k\n       right)\n            textexpand right-hand side of  eqrefpbident\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We now solve for overlineX:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noverlineX_ijk\n    = operatornameconjleft(\n            2 operatornamereal left( overlineOmega_i1k right)\n            operatornameconj left( X_ijk right)\n        right)\n    = 2operatornamereal left( overlineOmega_i1k right) X_ijk\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Like the frule, this rrule can be implemented generically:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(sum), ::typeof(abs2), X::Array{<:RealOrComplex}; dims = :)\n    function sum_abs2_pullback(ΔΩ)\n        ∂abs2 = DoesNotExist()\n        ∂X = @thunk(2 .* real.(ΔΩ) .* X)\n        return (NO_FIELDS, ∂abs2, ∂X)\n    end\n    return sum(abs2, X; dims = dims), sum_abs2_pullback\nend","category":"page"},{"location":"arrays.html#Functions-that-return-a-tuple","page":"Deriving Array Rules","title":"Functions that return a tuple","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Every Julia function returns a single output. For example, let's look at LinearAlgebra.logabsdet, the logarithm of the absolute value of the determinant of a matrix, which returns log det(A) and operatornamesign(det A) = fracdet A det A :","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"(l, s) = logabsdet(A)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The return type is actually a single output, a tuple of scalars, but when deriving, we treat them as multiple outputs. The left-hand side of \\eqref{pbident} then becomes a sum over terms, just like the right-hand side.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Let's derive the forward- and reverse-mode rules for logabsdet.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nl = log det(A)\ns = operatornamesign(det(A))\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where operatornamesign(x) = fracxx.","category":"page"},{"location":"arrays.html#Forward-mode-rule","page":"Deriving Array Rules","title":"Forward-mode rule","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"To make this easier, let's break the computation into more manageable steps:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nd = det(A)\na = d = sqrtoperatornamereal left( operatornameconj(d) d right)\nl = log a\ns = fracda\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We'll make frequent use of the identities:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"d = a s","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"operatornameconj(s) s = fracoperatornameconj(d) da^2 = fraca^2a^2 = 1","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"It will also be useful to define b = operatornametrleft( A^-1 dotA right).","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"For dotd, we use the pushforward for the determinant given in section 2.2.4 of [Giles2008ext]:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotd = d b","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Now we'll compute the pushforwards for the remaining steps.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\ndota = frac12 a fracddt\n                         operatornamerealleft( operatornameconj(d) d right)\n        = frac22 a operatornamereal left( operatornameconj(d) dotd right)\n        = operatornamereal left( operatornameconj(s) dotd right)\n             textuse  d = a s \n        = operatornamereal left( operatornameconj(s) d b right)\n             textsubstitute  dotd \ndotl = a^-1 dota\n        = a^-1 operatornamereal left( operatornameconj(s) d b right)\n             textsubstitute  dota\n        = operatornamereal left( operatornameconj(s) s b right)\n             textuse  d = a s \n        = operatornamereal left(b right)\n             textuse  operatornameconj(s) s = 1\ndots = a^-1 dotd - a^-2 d dota\n        = a^-1 left( dotd - dota s right)\n             textuse  d = a s \n        = a^-1 left(\n               dotd - operatornamereal left( operatornameconj(s) dotd right) s\n           right)\n             textsubstitute  dota\n        = a^-1 left(\n               dotd - left(\n                   operatornameconj(s) dotd -\n                   i operatornameimag left( operatornameconj(s) dotd right)\n               right) s\n           right)\n             textuse  operatornamereal(x) = x - i operatornameimag(x)\n        = a^-1 left(\n               dotd - left( operatornameconj(s) s right) dotd +\n               i operatornameimag left( operatornameconj(s) dotd right) s \n               right)\n        = i a^-1 operatornameimag left( operatornameconj(s) dotd right) s\n             textuse  operatornameconj(s) s = 1\n        = i a^-1 operatornameimag left( operatornameconj(s) d b right) s\n             textsubstitute  dotd\n        = i operatornameimag left( operatornameconj(s) s b right) s\n             textuse  d = a s \n        = i operatornameimag(b) s\n             textuse  operatornameconj(s) s = 1\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Note that the term b is reused. In summary, after all of that work, the final pushforward is quite simple:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\nb = operatornametr left( A^-1 dotA right) labellogabsdet_b \ndotl = operatornamereal(b) labellogabsdet_ldot\ndots = i operatornameimag(b) s labellogabsdet_sdot\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can define the frule as:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule((_, ΔA), ::typeof(logabsdet), A::Matrix{<:RealOrComplex})\n    # The primal function uses the lu decomposition to compute logabsdet\n    # we reuse this decomposition to compute inv(A) * ΔA\n    F = lu(A, check = false)\n    Ω = logabsdet(F)  # == logabsdet(A)\n    b = tr(F \\ ΔA)  # == tr(inv(A) * ΔA)\n    s = last(Ω)\n    ∂l = real(b)\n    # for real A, ∂s will always be zero (because imag(b) = 0)\n    # this is type-stable because the eltype is known\n    ∂s = eltype(A) <: Real ? Zero() : im * imag(b) * s\n    # tangents of tuples are of type Composite{<:Tuple}\n    ∂Ω = Composite{typeof(Ω)}(∂l, ∂s)\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Reverse-mode-rule","page":"Deriving Array Rules","title":"Reverse-mode rule","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noperatornamerealleft( operatornametrleft(\n    overlinel^H dotl\nright) right) +\noperatornamerealleft( operatornametrleft(\n    overlines^H dots\nright) right)\n     textleft-hand side of  eqrefpbidentmat\n= operatornamerealleft( \n       operatornameconj left( overlinel right) dotl +\n       operatornameconj left( overlines right) dots\n   right) \n= operatornamerealleft( \n       operatornameconj left( overlinel right) operatornamereal(b) +\n       i operatornameconj left( overlines right) s operatornameimag(b)\n   right)\n        textsubstitute  eqreflogabsdet_ldot text and  eqreflogabsdet_sdot \n= operatornamerealleft( \n       operatornamerealleft( overlinel right) operatornamereal(b) -\n       operatornameimag left(\n           operatornameconj left( overlines right) s\n       right) operatornameimag(b)\n   right)\n        textdiscard imaginary parts \n= operatornamerealleft(\n       left(\n           operatornamereal left( overlinel right) +\n           i operatornameimag left(\n               operatornameconj left( overlines right) s\n           right)\n       right) b\n   right)\n        textgather parts of  b \n= operatornamerealleft(\n       left(\n           operatornamereal left( overlinel right) +\n           i operatornameimag left(\n               operatornameconj left( overlines right) s\n           right)\n       right)\n       operatornametr(A^-1 dotA)\n   right)\n        textsubstitute  b text from  eqreflogabsdet_b \n= operatornamerealleft( operatornametr left(\n       left(\n           operatornamereal left( overlinel right) +\n           i operatornameimag left(\n               operatornameconj left( overlines right) s\n           right)\n       right)\n       A^-1 dotA\n   right) right)\n        textbring scalar within  operatornametr \n= operatornamerealleft( operatornametr left( overlineA^H dotA right) right)\n        textright-hand side of  eqrefpbidentmat\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Now we solve for overlineA:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noverlineA = left( left(\n    operatornamereal left( overlinel right) +\n    i operatornameimag left( operatornameconj left( overlines right) s right)\nright) A^-1 right)^H\n= left(\n    operatornamereal left( overlinel right) +\n    i operatornameimag left( operatornameconj left( s right) overlines right)\nright) A^-H\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The rrule can be implemented as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(logabsdet), A::Matrix{<:RealOrComplex})\n    # The primal function uses the lu decomposition to compute logabsdet\n    # we reuse this decomposition to compute inv(A)\n    F = lu(A, check = false)\n    Ω = logabsdet(F)  # == logabsdet(A)\n    s = last(Ω)\n    function logabsdet_pullback(ΔΩ)\n        (Δl, Δs) = ΔΩ\n        f = conj(s) * Δs\n        imagf = f - real(f)  # 0 for real A and Δs, im * imag(f) for complex A and/or Δs\n        g = real(Δl) + imagf\n        ∂A = g * inv(F)'  # == g * inv(A)'\n        return (NO_FIELDS, ∂A)\n    end\n    return (Ω, logabsdet_pullback)\nend","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"note: Note\nIt's a good idea when deriving pushforwards and pullbacks to verify that they make sense. For the pushforward, since l is real, it follows that dotl is too.What about dots? Well, s = fracdd is point on the unit circle in the complex plane. Multiplying a complex number by i rotates it counter-clockwise by 90°. So the expression for dots takes a real number, operatornameimag(b), multiplies by s to make it parallel to s, then multiplies by i to make it perpendicular to s, that is, perfectly tangent to the unit complex circle at s.For the pullback, it again follows that only the real part of overlinel is pulled back.operatornameconj(s) rotates a number parallel to s to the real line. So operatornameconj(s) overlines rotates overlines so that its imaginary part is the part that was tangent to the complex circle at s, while the real part is the part that was not tangent. Then the pullback isolates the imaginary part, which effectively is a projection. That is, any part of the adjoint overlines that is not tangent to the complex circle at s will not contribute to overlineA.","category":"page"},{"location":"arrays.html#More-examples","page":"Deriving Array Rules","title":"More examples","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"For more instructive examples of array rules, see [Giles2008ext] (real vector and matrix rules) and the LinearAlgebra rules in ChainRules.","category":"page"},{"location":"arrays.html#References","page":"Deriving Array Rules","title":"References","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"[Giles2008]: Giles M. B. Collected Matrix Derivative Results for Forward and Reverse Mode Algorithmic Differentiation. In: Advances in Automatic Differentiation. Lecture Notes in Computational Science and Engineering, vol 64: pp 35-44. Springer, Berlin (2008). doi: 10.1007/978-3-540-68942-3_4. pdf","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"[Giles2008ext]: Giles M. B. An Extended Collection of Matrix Derivative Results for Forward and Reverse Mode Algorithmic Differentiation. (unpublished). pdf","category":"page"},{"location":"index.html#ChainRules","page":"Introduction","title":"ChainRules","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ChainRules provides a variety of common utilities that can be used by downstream automatic differentiation (AD) tools to define and execute forward-, reverse-, and mixed-mode primitives.","category":"page"},{"location":"index.html#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ChainRules is all about providing a rich set of rules for differentiation. When a person learns introductory calculus, they learn that the derivative (with respect to x) of a*x is a, and the derivative of sin(x) is cos(x), etc. And they learn how to combine simple rules, via the chain rule, to differentiate complicated functions. ChainRules is a programmatic repository of that knowledge, with the generalizations to higher dimensions.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Autodiff (AD) tools roughly work by reducing a problem down to simple parts that they know the rules for, and then combining those rules. Knowing rules for more complicated functions speeds up the autodiff process as it doesn't have to break things down as much.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ChainRules is an AD-independent collection of rules to use in a differentiation system.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: The whole field is a mess for terminology\nIt isn't just ChainRules, it is everyone. Internally ChainRules tries to be consistent. Help with that is always welcomed.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: Primal\nOften we will talk about something as primal. That means it is related to the original problem, not its derivative. For example in y = foo(x), foo is the primal function, and computing foo(x) is doing the primal computation. y is the primal return, and x is a primal argument. typeof(y) and typeof(x) are both primal types.","category":"page"},{"location":"index.html#frule-and-rrule","page":"Introduction","title":"frule and rrule","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: `frule` and `rrule`\nfrule and rrule are ChainRules specific terms. Their exact functioning is fairly ChainRules specific, though other tools have similar functions. The core notion is sometimes called custom AD primitives, custom adjoints, custom gradients, custom sensitivities.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The rules are encoded as frules and rrules, for use in forward-mode and reverse-mode differentiation respectively.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The rrule for some function foo, which takes the positional arguments args and keyword arguments kwargs, is written:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function rrule(::typeof(foo), args...; kwargs...)\n    ...\n    return y, pullback\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"where y (the primal result) must be equal to foo(args...; kwargs...). pullback is a function to propagate the derivative information backwards at that point. That pullback function is used like: ∂self, ∂args... = pullback(Δy)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Almost always the pullback will be declared locally within the rrule, and will be a closure over some of the other arguments, and potentially over the primal result too.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The frule is written:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function frule((Δself, Δargs...), ::typeof(foo), args...; kwargs...)\n    ...\n    return y, ∂Y\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"where again y = foo(args; kwargs...), and ∂Y is the result of propagating the derivative information forwards at that point. This propagation is call the pushforward. Often we will think of the frule as having the primal computation y = foo(args...; kwargs...), and the pushforward ∂Y = pushforward(Δself, Δargs...), even though they are not present in seperate forms in the code.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: Why `rrule` returns a pullback but `frule` doesn't return a pushforward\nWhile rrule takes only the arguments to the original function (the primal arguments) and returns a function (the pullback) that operates with the derivative information, the frule does it all at once. This is because the frule fuses the primal computation and the pushforward. This is an optimization that allows frules to contain single large operations that perform both the primal computation and the pushforward at the same time (for example solving an ODE).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This operation is only possible in forward mode (where frule is used) because the derivative information needed by the pushforward available with the frule is invoked – it is about the primal function's inputs.     In contrast, in reverse mode the derivative information needed by the pullback is about the primal function's output.     Thus the reverse mode returns the pullback function which the caller (usually an AD system) keeps hold of until derivative information about the output is available.","category":"page"},{"location":"index.html#The-propagators:-pushforward-and-pullback","page":"Introduction","title":"The propagators: pushforward and pullback","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: pushforward and pullback\nPushforward and pullback are fancy words that the autodiff community recently adopted from Differential Geometry. The are broadly in agreement with the use of pullback and pushforward in differential geometry. But any geometer will tell you these are the super-boring flat cases. Some will also frown at you. They are also sometimes described in terms of the jacobian: The pushforward is jacobian vector product (jvp), and pullback is jacobian transpose vector product (j'vp). Other terms that may be used include for pullback the backpropagator, and by analogy for pushforward the forwardpropagator, thus these are the propagators. These are also good names because effectively they propagate wiggles and wobbles through them, via the chain rule. (the term backpropagator may originate with \"Lambda The Ultimate Backpropagator\" by Pearlmutter and Siskind, 2008)","category":"page"},{"location":"index.html#Core-Idea","page":"Introduction","title":"Core Idea","text":"","category":"section"},{"location":"index.html#Less-formally","page":"Introduction","title":"Less formally","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward takes a wiggle in the input space, and tells what wobble you would create in the output space, by passing it through the function.\nThe pullback takes wobbliness information with respect to the function's output, and tells the equivalent wobbliness with respect to the functions input.","category":"page"},{"location":"index.html#More-formally","page":"Introduction","title":"More formally","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward of f takes the sensitivity of the input of f to a quantity, and gives the sensitivity of the output of f to that quantity The pullback of f takes the sensitivity of a quantity to the output of f, and gives the sensitivity of that quantity to the input of f.","category":"page"},{"location":"index.html#Math","page":"Introduction","title":"Math","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This is all a bit simplified by talking in 1D.","category":"page"},{"location":"index.html#Lighter-Math","page":"Introduction","title":"Lighter Math","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For a chain of expressions:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"a = f(x)\nb = g(a)\nc = h(b)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pullback of g, which incorporates the knowledge of ∂b/∂a, applies the chain rule to go from ∂c/∂b to ∂c/∂a.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward of g,  which also incorporates the knowledge of ∂b/∂a, applies the chain rule to go from ∂a/∂x to ∂b/∂x.","category":"page"},{"location":"index.html#Geometric-interpretation-of-reverse-and-forwards-mode-AD","page":"Introduction","title":"Geometric interpretation of reverse and forwards mode AD","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let us think of our types geometrically. In other words, elements of a type form a manifold. This document will explain this point of view in some detail.","category":"page"},{"location":"index.html#Some-terminology/conventions","page":"Introduction","title":"Some terminology/conventions","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let p be an element of type M, which is defined by some assignment of numbers x_1x_m, say (x_1x_m) = (a_11_m)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"A function fM to K on M is (for simplicity) a polynomial Kx_1  x_m","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The tangent space T_pM of T at point p is the K-vector space spanned by derivations ddx. The tangent space acts linearly on the space of functions. They act as usual on functions. Our starting point is that we know how to write down ddx(f) = dfdx.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The collection of tangent spaces T_pM for pin M is called the tangent bundle of M.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let df denote the first order information of f at each point. This is called the differential of f. If the derivatives of f and g agree at p, we say that df and dg represent the same cotangent at p. The covectors dx_1  dx_m form the basis of the cotangent space T^*_pM at p. Notice that this vector space is dual to T_p","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The collection of cotangent spaces T^*_pM for pin M is called the cotangent bundle of M.","category":"page"},{"location":"index.html#Push-forwards-and-pullbacks","page":"Introduction","title":"Push-forwards and pullbacks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let N be another type, defined by numbers y_1y_n, and let gM to N be a map, that is, an n-dimensional vector (g_1  g_m) of functions on M.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"We define the push-forward g_*TM to TN between tangent bundles by g_*(X)(h) = X(gcirc h) for any tangent vector X and function f. We have g_*(ddx_i)(y_j) = dg_jdx_i, so the push-forward corresponds to the Jacobian, given a chosen basis.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Similarly, the pullback of the differential df is defined by g^*(df) = d(fcirc g). So for a coordinate differential dy_j, we have g^*(dy_j) = d(g_j). Notice that this is a covector, and we could have defined the pullback by its action on vectors by g^*(dh)(X) = g_*(X)(dh) = X(gcirc h) for any function f on N and Xin TM. In particular, g^*(dy_j)(ddx_i) = d(g_j)dx_i. If you work out the action in a basis of the cotangent space, you see that it acts by the adjoint of the Jacobian.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Notice that the pullback of a differential and the pushforward of a vector have a very different meaning, and this should be reflected on how they are used in code.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The information contained in the push-forward map is exactly what does my function do to tangent vectors. Pullbacks, acting on differentials of functions, act by taking the total derivative of a function. This works in a coordinate invariant way, and works without the notion of a metric. Gradients recall are vectors, yet they should contain the same information of the differential df. Assuming we use the standard euclidean metric, we can identify df and nabla f as vectors. But pulling back gradients still should not be a thing.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"If the goal is to evaluate the gradient of a function f=gcirc hM to N to K, where g is a map and h is a function, we have two obvious options: First, we may push-forward a basis of M to TK which we identify with K itself. This results in m scalars, representing components of the gradient. Step-by-step in coordinates:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Compute the push-forward of the basis of T_pM, i.e. just the columns of the Jacobian dg_idx_j.\nCompute the push-forward of the function h (consider it as a map, K is also a manifold!) to get h_*(g_*T_pM) = sum_j dhdy_i (dg_idx_j)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Second, we pull back the differential dh:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"compute dh = dhdy_1dhdy_n in coordinates.\npull back by (in coordinates) multiplying with the adjoint of the Jacobian, resulting in g_*(dh) = sum_i(dg_idx_j)(dhdy_i).","category":"page"},{"location":"index.html#The-anatomy-of-pullback-and-pushforward","page":"Introduction","title":"The anatomy of pullback and pushforward","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For our function foo(args...; kwargs...) = y:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function pullback(Δy)\n    ...\n    return ∂self, ∂args...\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The input to the pullback is often called the seed. If the function is y = f(x) often the pullback will be written s̄elf, x̄ = pullback(ȳ).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: Note\nThe pullback returns one ∂arg per arg to the original function, plus one ∂self for the fields of the function itself (explained below).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: perturbation, seed, sensitivity\nSometimes perturbation, seed, and even sensitivity will be used interchangeably. They are not generally synonymous, and ChainRules shouldn't mix them up. One must be careful when reading literature. At the end of the day, they are all wiggles or wobbles.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward is a part of the frule function. Considered alone it would look like:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function pushforward(Δself, Δargs...)\n    ...\n    return ∂y\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"But because it is fused into frule we see it as part of:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function frule((Δself, Δargs...), ::typeof(foo), args...; kwargs...)\n    ...\n    return y, ∂y\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The input to the pushforward is often called the perturbation. If the function is y = f(x) often the pushforward will be written ẏ = last(frule((ṡelf, ẋ), f, x)). ẏ is commonly used to represent the perturbation for y.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: Note\nIn the frule/pushforward, there is one Δarg per arg to the original function. The Δargs are similar in type/structure to the corresponding inputs args (Δself is explained below). The ∂y are similar in type/structure to the original function's output Y. In particular if that function returned a tuple then ∂y will be a tuple of the same size.","category":"page"},{"location":"index.html#Self-derivative-Δself,-self,-self,-ṡelf-etc","page":"Introduction","title":"Self derivative Δself, ∂self, s̄elf, ṡelf etc","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: Δself, ∂self, s̄elf, ṡelf\nIt is the derivatives with respect to the internal fields of the function. To the best of our knowledge there is no standard terminology for this. Other good names might be Δinternal/∂internal.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"From the mathematical perspective, one may have been wondering what all this Δself, ∂self is. Given that a function with two inputs, say f(a, b), only has two partial derivatives: dfracfa, dfracfb. Why then does a pushforward take in this extra Δself, and why does a pullback return this extra ∂self?","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The reason is that in Julia the function f may itself have internal fields. For example a closure has the fields it closes over; a callable object (i.e. a functor) like a Flux.Dense has the fields of that object.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Thus every function is treated as having the extra implicit argument self, which captures those fields. So every pushforward takes in an extra argument, which is ignored unless the original function has fields. It is common to write function foo_pushforward(_, Δargs...) in the case when foo does not have fields. Similarly every pullback returns an extra ∂self, which for things without fields is the constant NO_FIELDS, indicating there are no fields within the function itself.","category":"page"},{"location":"index.html#Pushforward-/-Pullback-summary","page":"Introduction","title":"Pushforward / Pullback summary","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Pullback\nreturned by rrule\ntakes output space wobbles, gives input space wiggles\nArgument structure matches structure of primal function output\nIf primal function returns a tuple, then pullback takes in a tuple of differentials.\n1 return per original function argument + 1 for the function itself\nPushforward:\npart of frule\ntakes input space wiggles, gives output space wobbles\nArgument structure matches primal function argument structure, but passed as a tuple at start of frule\n1 argument per original function argument + 1 for the function itself\n1 return per original function return","category":"page"},{"location":"index.html#Pullback/Pushforward-and-Directional-Derivative/Gradient","page":"Introduction","title":"Pullback/Pushforward and Directional Derivative/Gradient","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The most trivial use of the pushforward from within frule is to calculate the directional derivative:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"If we would like to know the directional derivative of f for an input change of (1.5, 0.4, -1)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"direction = (1.5, 0.4, -1) # (ȧ, ḃ, ċ)\ny, ẏ = frule((Zero(), direction...), f, a, b, c)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"On the basis directions one gets the partial derivatives of y:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"y, ∂y_∂a = frule((Zero(), 1, 0, 0), f, a, b, c)\ny, ∂y_∂b = frule((Zero(), 0, 1, 0), f, a, b, c)\ny, ∂y_∂c = frule((Zero(), 0, 0, 1), f, a, b, c)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Similarly, the most trivial use of rrule and returned pullback is to calculate the gradient:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"y, f_pullback = rrule(f, a, b, c)\n∇f = f_pullback(1)  # for appropriate `1`-like seed.\ns̄elf, ā, b̄, c̄ = ∇f","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Then we have that ∇f is the gradient of f at (a, b, c). And we thus have the partial derivatives overlinemathrmself = dfracfmathrmself, overlinea = dfracfa, overlineb = dfracfb, overlinec = dfracfc, including the and the self-partial derivative, overlinemathrmself.","category":"page"},{"location":"index.html#Differentials","page":"Introduction","title":"Differentials","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The values that come back from pullbacks or pushforwards are not always the same type as the input/outputs of the primal function. They are differentials, which correspond roughly to something able to represent the difference between two values of the primal types. A differential might be such a regular type, like a Number, or a Matrix, matching to the original type; or it might be one of the AbstractDifferential subtypes.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Differentials support a number of operations. Most importantly: + and *, which let them act as mathematical objects.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The most important AbstractDifferentials when getting started are the ones about avoiding work:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Thunk: this is a deferred computation. A thunk is a word for a zero argument closure. A computation wrapped in a @thunk doesn't get evaluated until unthunk is called on the thunk. unthunk is a no-op on non-thunked inputs.\nOne, Zero: There are special representations of 1 and 0. They do great things around avoiding expanding Thunks in multiplication and (for Zero) addition.","category":"page"},{"location":"index.html#Other-AbstractDifferentials:","page":"Introduction","title":"Other AbstractDifferentials:","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Composite{P}: this is the differential for tuples and  structs. Use it like a Tuple or NamedTuple. The type parameter P is for the primal type.\nDoesNotExist: Zero-like, represents that the operation on this input is not differentiable. Its primal type is normally Integer or Bool.\nInplaceableThunk: it is like a Thunk but it can do in-place add!.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"index.html#Example-of-using-ChainRules-directly","page":"Introduction","title":"Example of using ChainRules directly","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"While ChainRules is largely intended as a backend for autodiff systems, it can be used directly. In fact, this can be very useful if you can constrain the code you need to differentiate to only use things that have rules defined for. This was once how all neural network code worked.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Using ChainRules directly also helps get a feel for it.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using ChainRulesCore\n\nfunction foo(x)\n    a = sin(x)\n    b = 0.2 + a\n    c = asin(b)\n    return c\nend\n\n# Define rules (alternatively get them for free via `using ChainRules`)\n@scalar_rule(sin(x), cos(x))\n@scalar_rule(+(x, y), (One(), One()))\n@scalar_rule(asin(x), inv(sqrt(1 - x^2)))","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via rrules\n#### First the forward pass, gathering up the pullbacks\nx = 3;\na, a_pullback = rrule(sin, x);\nb, b_pullback = rrule(+, 0.2, a);\nc, c_pullback = rrule(asin, b)\n\n#### Then the backward pass calculating gradients\nc̄ = 1;                    # ∂c/∂c\n_, b̄ = c_pullback(c̄);     # ∂c/∂b = ∂c/∂b ⋅ ∂c/∂c\n_, _, ā = b_pullback(b̄);  # ∂c/∂a = ∂c/∂b ⋅ ∂b/∂a\n_, x̄ = a_pullback(ā);     # ∂c/∂x = ∂c/∂a ⋅ ∂a/∂x\nx̄                         # ∂c/∂x = ∂foo/∂x\n# output\n-1.0531613736418153","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via frules\nx = 3;\nẋ = 1;              # ∂x/∂x\nnofields = Zero();  # ∂self/∂self\n\na, ȧ = frule((nofields, ẋ), sin, x);             # ∂a/∂x = ∂a/∂x ⋅ ∂x/∂x \nb, ḃ = frule((nofields, Zero(), ȧ), +, 0.2, a);  # ∂b/∂x = ∂b/∂a ⋅ ∂a/∂x\nc, ċ = frule((nofields, ḃ), asin, b);            # ∂c/∂x = ∂c/∂b ⋅ ∂b/∂x\nċ                                                # ∂c/∂x = ∂foo/∂x\n# output\n-1.0531613736418153","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via FiniteDifferences.jl\nusing FiniteDifferences\ncentral_fdm(5, 1)(foo, x)\n# output\n-1.0531613736418257\n\n#### Find dfoo/dx via ForwardDiff.jl\nusing ForwardDiff\nForwardDiff.derivative(foo, x)\n# output\n-1.0531613736418153\n\n#### Find dfoo/dx via Zygote.jl\nusing Zygote\nZygote.gradient(foo, x)\n# output\n(-1.0531613736418153,)","category":"page"}]
}
